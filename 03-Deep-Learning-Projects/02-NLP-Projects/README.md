# Natural Language Processing Projects

## Project Description
This section explores various Natural Language Processing (NLP) applications using deep learning techniques. The projects range from text classification and sentiment analysis to creative text generation tasks, demonstrating different approaches to processing and generating human language.

## Learning Outcomes
- Understanding text preprocessing and tokenization
- Implementing word embeddings and sequence models
- Building text classification pipelines
- Developing generative language models
- Applying attention mechanisms and transformers

## Project Structure
1. **01-News-Classification**
   - Multi-class text classification
   - Text preprocessing techniques
   - Document embedding methods
   - Model evaluation strategies

2. **02-Sentiment-Analysis-IMDB**
   - Binary sentiment classification
   - Word embedding implementation
   - RNN/LSTM architectures
   - Performance optimization

3. **03-Text-Generation-Shakespeare**
   - Character-level text generation
   - Sequence modeling
   - Temperature sampling
   - Creative text generation

4. **04-Poetry-Generation-LSTM**
   - Advanced text generation
   - LSTM networks
   - Literary style transfer
   - Creative writing applications

## Technologies and Libraries
- TensorFlow 2.x
- NLTK
- spaCy
- Keras Text Processing
- Word2Vec/GloVe
- TensorFlow Hub

## Results and Metrics
- News Classification: >92% categorical accuracy
- Sentiment Analysis: >88% binary accuracy
- Text Generation: Coherent sequence generation
- Poetry Generation: Style-consistent creative text

## References
- [TensorFlow Text Tutorial](https://www.tensorflow.org/tutorials/text/text_classification)
- [Natural Language Processing with Deep Learning](https://web.stanford.edu/class/cs224n/)
- [LSTM Networks for Text Generation](https://www.tensorflow.org/text/tutorials/text_generation)
- [Attention Is All You Need Paper](https://arxiv.org/abs/1706.03762) 